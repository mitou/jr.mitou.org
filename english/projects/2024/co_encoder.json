{"@context":"https://schema.org","@type":"CreativeWork","@id":"https://jr.mitou.org/english/projects/2024/co_encoder","name":"Co-Encoder - Context Compression Encoder for LLMs","description":"In recent years, the technology of generative AI has advanced significantly, enabling large language models (LLMs) like ChatGPT to generate natural text. However, these models face the challenge of high inference costs. Particularly when processing long texts, they require substantial memory and high-performance GPUs, making inference difficult with standard GPUs. To address this issue, we propose a Co-Encoder that compresses input converted into matrices and directly feeds it to the LLM, enabling low-cost inference of long texts.","creator":[{"@type":"Person","name":"Rakuto Suda"}],"contributor":{"@type":"Person","name":"Hirokazu Nishio","url":"https://jr.mitou.org/mentors/#hirokazu_nishio"},"video":{"@type":"VideoObject","name":"Co-Encoder - Context Compression Encoder for LLMs","embedUrl":"https://www.youtube.com/embed/El3ciW2be3s","description":"In recent years, the technology of generative AI has advanced significantly, enabling large language models (LLMs) like ChatGPT to generate natural text. However, these models face the challenge of high inference costs. Particularly when processing long texts, they require substantial memory and high-performance GPUs, making inference difficult with standard GPUs. To address this issue, we propose a Co-Encoder that compresses input converted into matrices and directly feeds it to the LLM, enabling low-cost inference of long texts.","thumbnailUrl":"/assets/img/projects/2024/co_encoder.webp","uploadDate":"2024-11-03T10:00:00+09:00"},"dateCreated":"2024-10-01T10:00:00+09:00","educationalLevel":"Primary and Secondary Education (Ages 17 and under)","inLanguage":"en","isPartOf":{"@type":"EducationalProgram","name":"MITOU Junior","url":"https://jr.mitou.org"}}